# Evaluation und Bewertung

## Hochverfügbares Elixir vs Single Node
Immer mehr Alltagsanwendungen sind heutzutage über das Internet via Webbrowser erreichbar. Diese "Software as a Service" Produkte müssen dabei hohen Anforderungen gerecht werden. Im Mittelpunkt stehen beispielsweise Eigenschaften wie "Zero Downtime" und hohe Reaktionszeiten der Software. Das Synonym "Zero Downtime" bedeutet dabei, dass die Anwendung jederzeit verfügbar ist, also auch wenn nebenbei Software-Updates stattfinden oder die Datenbank migriert wird. Hohe Reaktionszeiten sind dabei maßgeblich für eine gute "User-Experience" verantwortlich. Diese ist wiederum eng verknüpft mit der durch den Benutzenden wahrgenommenen Qualität der Anwendung. Diese Hohe Software-Verfügbarkeit von verteilten Anwendungen kann dabei über zwei Wege erreicht werden. Man kann zum einen vertikal skalieren, indem man die Instanz, auf welcher die Anwendung läuft, mit mehr Ressourcen ausstattet. Oder man kann zum anderen horizontal skalieren, indem man einfach mehr Instanzen der Anwendung, sogenannte Replikationen, parallel betreibt. 

Genau an diesem Punkt kommt Kubernetes ins Spiel. Kubernetes ist genau für eine solche horizontale Skalierung ausgelegt. In den zuvor beschriebenen Deployment-Konfiguration kann angegeben werden, wie viele Replikas der Anwendung parallel laufen sollen. Dies wird dadurch bewerkstelligt, dass von der Anwendungskonfiguration sogenannte "Blueprints", also exakte "Klone" der Anwendung gestartet werden, welche dann über verschiedene IP-Adressen erreichbar sind. In diesen Anwendungskonfigurationen wiederum wird das container-Image der Anwendung angegeben, welches beispielsweise ein Docker Image sein kann, wie in der im Prototypen verwendeten Anwendung. Hällt man sich an die Namensgebung von Kubernetes, laufen die Anwendungen dann in sogenannten Pods. Ein [Pod](https://kubernetes.io/de/docs/concepts/workloads/pods/) ist also eine Abstraktion von einem oder mehreren Containern, welche gemeinsam dieselben Speicher- und Netzwerkressourcen nutzen, sowie eine gemeinsame Spezifikation für die Ausführung dieser Container teilen. Ein sogenannter LoadBalancer entscheidet dann beispielsweise anhand der aktuellen Auslastung der Instanzen, auf welche Instanz ein neuer Client weitergeleitet wird. Nun fehlt nur noch die Erklärung der zwei Fachbegriffe "Node" und "Cluster" um, auf die Hochverfügbarkeit von Elixir ansich eingehen zu können. Kubernetes führt "workload" bzw. Arbeitslast aus, indem es Container in Pods platziert, die anschließend auf ["Nodes"](https://kubernetes.io/docs/concepts/architecture/nodes/) ausgeführt werden. Jeder dieser Knoten wird von der Steuerungsebene verwaltet und bündelt die für den Betrieb von Pods erforderlichen Dienste. Normalerweise werden dann mehrere dieser Knoten in einem Cluster, also einem Verbund bzw. einer Gruppe dieser Knoten betrieben, auf denen von Kubernetes verwaltete containerisierte Anwendungen laufen.

Elixir-Anwendungen lassen sich perfekt in dieses Konzept integrieren. Elixir besitzt dabei alle notwendigen verteilten Funktionen, welche benötigt werden, um ein Cluster zwischen den verschiedenen Instanzen der jeweiligen Anwendungen aufbauen zu können. Dafür werden keine zusätzlichen Abhängigkeiten benötigt. Um so ein Cluster aufzusetzen, muss jeder Anwendungsinstanz ein Name zugewiesen werden, welcher bestimmte Konventionen erfüllen muss. Anschließend können die erzeugten Knoten miteinander verbunden werden. Dies läuft über Angabe eines festgelegten eindeutigen Namens via der ["connect"](https://hexdocs.pm/elixir/1.12/Node.html#connect/1) Funktion des Elixir-Node-Moduls. Der Name muss dabei folgendes Format haben \<app-name\>@\<node-ip\>. Ist die IP-Adresse immer dieselbe, muss jedoch der \<app-name\> den Knoten eindeutig identifizieren, also variieren. Dieser kann im Kubernetes Deployment über das Setzen der Umgebungsvariable [RELEASE_NODE](https://hexdocs.pm/mix/Mix.Tasks.Release.html) dynamisch zugewiesen werden. Um nicht bei jedem Start eines neuen Knotens manuell die genannte "connect" Funktion aufrufen zu müssen, um Cluster zu bilden, gibt es die Elixir-Bibliothek [libcluster](https://hexdocs.pm/libcluster/readme.html). Sie erlaubt die automatische Clusterbildung aus Erlangknoten. Dies kann dynamisch oder statisch passieren. Dynamisch bedeutet dabei, dass sich die Knoten automatisch über einen Erkennungsmechanismus wie beispielsweise DNS finden und formieren können. Um dies zu erreichen, kann eine Vielzahl von verschiedenen Formierungsstrategien wie [Kubernetes DNS](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service/) oder [epmd](https://www.erlang.org/doc/man/epmd.html) genutzt werden. Der Erlang Epmd-Deamon fungiert dabei als Namenserver in verteilten Erlang und damit auch Elixir Umgebungen. Damit man an dieser Stelle nicht an die Implementierung von Kubernetes gebunden ist, wird eine Schnittstelle mit dem Namen "Headless Services" für eigene Service-Erkennungsmechanismen bereitgestellt. Diese Bibliothek wurde auch beim Aufbau des Prototypen dieser Arbeit verwendet. In Abbildung 1 sind die hier vorgestellten Konzepte auch noch mal bildlich zu sehen.

![Elixir-Node-Cluster](https://github.com/Elixir2K8s/docs/blob/main/Libcluster.png)

Abbildung 1

## Hochverfügbare Datenbank vs einzelne Instanz

Nachdem die Elixir-Anwendung hochverfügbar gemacht wurde, ist die Postgres-Datenbank der letzte verbleibende Single Point of Failure, innerhalb des Kubernetes-Clusters.

Um diesen zu Eliminieren muss die Datenbank hochverfügbar gemacht werden. Hierfür sind mindestens zwei PostgresDB Pods erforderlich, wobei einer als Primary und die restlichen als Replica arbeiten. Für das Load Balancing und Failover zwischen den DB Pods kommt pgpool-II zum Einsatz. Das Failover zwischen Primary und Replicas wird durch den postgresql-repmgr verwaltet. Dieser promotet eine Replica zum neuen Primary, falls der bestehende Primary ausfällt.

Das [HA Postgres Setup von Bitnami](https://github.com/bitnami/charts/tree/master/bitnami/postgresql-ha) besteht aus einer Sammlung von K8s Service Definitions, die über einen Helm Chart installiert werden. Helm ist im übertragenen Sinne ein Paketmanager für Kubernetes Service Definitions, die eine Anwendung deployen. Um Helm in MicroK8s zu nutzen, muss Helm erst als Addon aktiviert werden.

Nachdem die Parameter des [Bitnami Postgres HA Helm Charts](https://github.com/bitnami/charts/tree/master/bitnami/postgresql-ha#global-parameters) wie gewünscht konfiguriert wurden kann dieser via Helm installiert werden und ist danach einsatzbereit.

Im Vergleich zur Variante mit einer einzelnen Postgres Instanz ist dies ein signifikanter Mehraufwand. bei der Konfiguration und Deployment, da man zusätzlich zu den eigenen Service Definitions einen Helm Chart hat, dessen Konfiguration es auch entsprechend zu warten gilt. Bei der einzelnen Datenbank hingegen kann das Deployment einfach zusammen mit der Elixir-Anwendung mit dem selben Befehl erfolgen, da die Service Definitions im gleichen Verzeichnis liegen. Auch bei der Konfiguration war durchaus weniger zu beachten, da die Service Definitions für die einzelne PostgresDB mit kompose aus der docker-compose Datei generiert werden konnten.

Betrachtet man den Aspekt der Hochverfügbarkeit bzw. Ausfallsicherheit ist es durchaus sinnvoll auf einen hochverfügbaren Postgres Helm Chart zu setzen, da einzelne Nodes im Kubernetes Cluster ausfallen können und dies bei einer einzelnen Datenbank zu einem temporären Komplettausfall der Anwendung führen würde. Da im Rahmen des Praktikums jedoch nur ein Single Node Kubernetes Cluster genutzt wird, bietet der Einsatz des Postgres HA Charts keine Vorteile unter dem Aspekt der physischen Ausfallsicherheit. Der einzige nennenswerte Vorteil des HA DB Setups in Kombination mit einem Single Node K8s Cluster ist die unterbrechungsfreie Upgrademöglichkeit der Datenbank, jedoch ist diese Funktionalität für ein Test Deployment vernachlässigbar.

Daher bleiben wir für das Test Deployment auf einem Single Node K8s Cluster bei einer einzelnen Postgres-Instanz.

